{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks shape: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "landmarks_test = pd.read_csv('fdata/fashion-mnist_test.csv')\n",
    "landmarks_train = pd.read_csv('fdata/fashion-mnist_train.csv')\n",
    "\n",
    "n = 600\n",
    "\n",
    "landmarks = landmarks_train.iloc[n, 1:].values.reshape(1,28,28)\n",
    "landmarks1 = landmarks_train.iloc[n, 0].reshape(1)\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "#print('First 4 Landmarks: {}'.format(landmarks[4,5]))\n",
    "#plt.imshow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "a = torch.Tensor(landmarks)\n",
    "#a = F.to_pil_image(landmarks)\n",
    "#landmarks = np.array(landmarks)\n",
    "#landmarks.dtype\n",
    "a = F.to_pil_image(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, transform1=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.transform1 = transform1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        image = torch.Tensor(self.landmarks_frame.iloc[idx, 1:].values.reshape(1,28,28))\n",
    "        label = self.landmarks_frame.iloc[idx, 0].reshape(1)\n",
    "        label = torch.LongTensor(label)\n",
    "        image =self.transform1(image)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        label = label[0]\n",
    "        landmarks = (image, label)\n",
    "        \n",
    "        return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = FashionMNIST(csv_file='fdata/fashion-mnist_train.csv',\n",
    "                             root_dir='fdata/',\n",
    "                             transform=transform,\n",
    "                             transform1=transforms.ToPILImage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FashionMNIST(csv_file='fdata/fashion-mnist_test.csv',\n",
    "                            root_dir='fdata/',\n",
    "                            transform=transform,\n",
    "                            transform1=transforms.ToPILImage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = 10\n",
    "\n",
    "train_load = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        nn.init.xavier_uniform(self.cnn1.weight)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.dr1 = nn.Dropout2d(0.07)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        nn.init.xavier_uniform(self.cnn2.weight)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        #self.dr2 = nn.Dropout2d(0.07)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        nn.init.xavier_uniform(self.cnn3.weight)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        #self.dr3 = nn.Dropout2d(0.07)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        #self.cnn4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #nn.init.xavier_uniform(self.cnn4.weight)\n",
    "        #self.bn4 = nn.BatchNorm1d(32)\n",
    "        #self.dr4 = nn.Dropout2d(0.07)\n",
    "        #self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Linear(32*7*7, 10)\n",
    "        self.bn5 = nn.BatchNorm1d(10)\n",
    "        nn.init.xavier_uniform(self.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.cnn1(x)\n",
    "        out = self.bn1(out)\n",
    "        #out = self.dr1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        out = self.cnn2(out)\n",
    "        out = self.bn2(out)\n",
    "        #out = self.dr2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        out = self.cnn3(out)\n",
    "        out = self.bn3(out)\n",
    "        #out = self.dr3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        #out = self.cnn4(out)\n",
    "        #out = self.bn4(out)\n",
    "        #out = self.dr4(out)\n",
    "        #out = self.relu4(out)\n",
    "        \n",
    "        out = out.view(out.size(0),-1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = self.bn5(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "learing_rate = 0.05\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learing_rate)\n",
    "\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 500, loss 0.4046255946159363, accuracy 88.11\n",
      "iterations 1000, loss 0.31093156337738037, accuracy 89.78\n",
      "iterations 1500, loss 0.3070923984050751, accuracy 90.33\n",
      "iterations 2000, loss 0.31821033358573914, accuracy 90.41\n",
      "iterations 2500, loss 0.2511518597602844, accuracy 90.78\n",
      "iterations 3000, loss 0.29597175121307373, accuracy 91.03\n",
      "iterations 3500, loss 0.18674243986606598, accuracy 91.21000000000001\n",
      "iterations 4000, loss 0.1667010635137558, accuracy 90.78\n",
      "iterations 4500, loss 0.07950937002897263, accuracy 90.97\n",
      "iterations 5000, loss 0.2118791788816452, accuracy 90.9\n",
      "iterations 5500, loss 0.10980124771595001, accuracy 90.73\n",
      "iterations 6000, loss 0.12645336985588074, accuracy 90.69\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images, labels) in enumerate(train_load):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if (iter%500) == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for images, labels in test_load:\n",
    "                images = Variable(images)\n",
    "            \n",
    "                outputs = model(images)\n",
    "                _, predicts = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicts == labels).sum()\n",
    "            accuracy = 100 * (correct.item() / total)\n",
    "                \n",
    "            print('iterations {}, loss {}, accuracy {}'.format(iter, loss.data[0], accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
